# Octup EÂ²A - Complete Technical Documentation

## ğŸ“‹ Table of Contents

1. [System Overview](#system-overview)
2. [Architecture](#architecture)
3. [Infrastructure](#infrastructure)
4. [Data Flow](#data-flow)
5. [Orchestration](#orchestration)
6. [System Components](#system-components)
7. [Deployment](#deployment)
8. [Monitoring](#monitoring)

---

## ğŸ¯ System Overview

**Octup EÂ²A (Exception to Action)** is a comprehensive platform for SLA monitoring and exception management in e-commerce logistics operations. The system provides:

- **Real-time SLA monitoring** for orders and operations
- **AI-powered exception analysis** with automated resolution
- **Automated billing** based on completed operations
- **Comprehensive business process orchestration** via Prefect
- **Scalable architecture** for high-load operations

### Key Features

- ğŸ”„ **Webhook-driven architecture** for real-time event processing
- ğŸ¤– **AI-powered analysis** of exceptions with automated resolution
- ğŸ“Š **Comprehensive analytics** and reporting
- ğŸ—ï¸ **Microservices architecture** with Docker containerization
- ğŸ” **Complete observability** with tracing, metrics, and logging
- ğŸ›¡ï¸ **Enterprise-grade security** with multi-tenancy

---

## ğŸ—ï¸ Architecture

### High-Level System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Shopify Mock  â”‚â”€â”€â”€â–¶â”‚   FastAPI App   â”‚â”€â”€â”€â–¶â”‚  Prefect Flows  â”‚
â”‚   (Data Source) â”‚    â”‚  (Event Proc.)  â”‚    â”‚ (Business Logic)â”‚
â”‚   Port: 8090    â”‚    â”‚   Port: 8000    â”‚    â”‚   Port: 4200    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â–¼                       â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚   PostgreSQL    â”‚    â”‚   Redis Cache   â”‚
         â”‚              â”‚   (Supabase)    â”‚    â”‚   (Local/Cloud) â”‚
         â”‚              â”‚   Port: 54322   â”‚    â”‚   Port: 6379    â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Next.js Dashboard â”‚  â”‚  Observability  â”‚    â”‚   AI Services   â”‚
â”‚   (Frontend)    â”‚    â”‚ (NewRelic/OTEL) â”‚    â”‚  (OpenRouter)   â”‚
â”‚   Port: 3000    â”‚    â”‚   Tracing/Logs  â”‚    â”‚  Gemini 2.0     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Architecture

#### 1. **Data Sources & Event Generation**
- **Shopify Mock API** (`demo/shopify-mock/`) - FastAPI-based simulator
  - Generates realistic orders with configurable problems (15% probability)
  - Produces 1001-1999 orders per batch (configurable via env vars)
  - Webhook delivery with 2-second delay for realism
  - Problem types: `delayed_fulfillment`, `inventory_shortage`, `shipping_delay`, `payment_issue`, `address_validation`

#### 2. **Event Processing Layer**
- **FastAPI Application** (`app/main.py`) - Main API server with comprehensive middleware
  - **Middleware Stack**: CORS â†’ Correlation â†’ Tenancy â†’ OpenTelemetry
  - **Event Ingestion**: Redis-based idempotency, real-time SLA evaluation
  - **Dead Letter Queue**: Failed event processing with replay capability
  - **WebSocket Support**: Real-time dashboard updates

#### 3. **Business Logic Orchestration**
- **Prefect 3.0 Flows** (`flows/`) - Comprehensive workflow orchestration
  - **Business Operations Orchestrator**: Master flow coordinating all operations
  - **Order Processing Flow**: Order fulfillment monitoring and stage processing
  - **Exception Management Flow**: AI-powered exception analysis and resolution
  - **Billing Management Flow**: Automated invoice generation and validation
  - **Data Enrichment Flow**: Analytics data preparation and trend analysis

#### 4. **Data Storage & Caching**
- **PostgreSQL (Supabase)** - Primary database with comprehensive schema
  - Tables: `tenants`, `order_events`, `exceptions`, `invoices`, `order_processing_stages`
  - Foreign key constraints and optimized indexes
  - Alembic migrations for schema management
- **Redis** - Caching and state management
  - SLA configuration caching (1-hour TTL)
  - Idempotency tracking
  - Dead Letter Queue storage

#### 5. **AI Integration**
- **OpenRouter API** with Gemini 2.0 Flash model
  - Exception classification and analysis
  - Automated resolution recommendations
  - Cost monitoring (200,000 tokens/day limit)
  - Circuit breaker pattern with fallback to rule-based logic

#### 6. **User Interface**
- **Next.js Dashboard** (`dashboard/`) - Modern React-based interface
  - Real-time metrics and exception management
  - WebSocket integration for live updates
  - Responsive design with Tailwind CSS and Radix UI components

---

## ğŸš€ Infrastructure

### Docker Compose Architecture

The system is deployed via Docker Compose (`docker/docker-compose.yml`) with the following services:

```yaml
services:
  # Main API Service
  api:
    - FastAPI application on port 8000
    - Event processing and API endpoints
    - Health checks every 30 seconds
    - Volume mounts for hot reloading in development

  # Shopify Mock API (Demo Profile)
  shopify-mock:
    - FastAPI-based Shopify simulator on port 8090
    - Configurable order generation (1001-1999 per batch)
    - Webhook delivery with 2-second delay
    - Health monitoring and dependency on API service

  # Next.js Dashboard (Dashboard Profile)
  dashboard:
    - React/Next.js application on port 3000
    - Production build with optimized assets
    - Environment-based API URL configuration

  # Prefect Server (Prefect Profile)
  prefect-server:
    - Prefect 3.0 community edition on port 4200
    - SQLite backend for development
    - UI and API endpoints for workflow management

  # Prefect Worker (Prefect Profile)
  prefect-worker:
    - Process-based worker for flow execution
    - Connects to default-agent-pool
    - Access to application code via volume mounts

  # Local Redis (Local-Redis Profile)
  redis-local:
    - Redis 7 Alpine on port 6379
    - 512MB memory limit with LRU eviction
    - Persistent storage for development

  # OTEL Collector (Observability Profile)
  otel-collector:
    - OpenTelemetry collector for trace aggregation
    - OTLP gRPC (4317) and HTTP (4318) receivers
    - Prometheus metrics endpoint (8888)
```

### External Dependencies

#### 1. **Supabase (PostgreSQL)**
- **Purpose**: Primary database with full PostgreSQL compatibility
- **Connection**: `postgresql+asyncpg://postgres:postgres@host.docker.internal:54322/postgres`
- **Schema**: Comprehensive schema with foreign key constraints and indexes
- **Management**: Supabase CLI for local development, cloud for production
- **Migrations**: Alembic-based schema versioning

#### 2. **Redis (Cloud/Local)**
- **Purpose**: Caching, idempotency tracking, and Dead Letter Queue
- **Configuration**: Via `REDIS_URL` environment variable
- **Usage**: SLA config caching (1-hour TTL), event deduplication, DLQ storage
- **Fallback**: Local Redis container for development

#### 3. **AI Services (OpenRouter)**
- **Purpose**: Exception analysis and automated resolution recommendations
- **Model**: `google/gemini-2.0-flash-exp:free` (200,000 tokens/day limit)
- **Integration**: Circuit breaker pattern with fallback to rule-based logic
- **Cost Monitoring**: Token usage tracking and daily limit enforcement

#### 4. **Observability Stack**
- **NewRelic**: Primary observability platform for production
- **OpenTelemetry**: Standardized tracing and metrics collection
- **Prometheus**: Metrics scraping and alerting (development)
- **Structured Logging**: JSON-formatted logs with correlation IDs

### Service Profiles

The Docker Compose configuration uses profiles for selective service deployment:

- **Default**: `api`, `prefect-server`, `prefect-worker`
- **`demo`**: Adds `shopify-mock` for data generation
- **`dashboard`**: Adds `dashboard` for web interface
- **`local-redis`**: Adds `redis-local` for development
- **`observability`**: Adds `otel-collector` for trace collection

---

## ğŸ”„ Data Flow

### 1. Event Generation and Processing Pipeline

```
Shopify Mock â”€â”€webhookâ”€â”€â–¶ FastAPI /events â”€â”€validationâ”€â”€â–¶ Database
     â”‚                        â”‚                              â”‚
     â”‚                        â–¼                              â”‚
     â”‚                  SLA Evaluation                       â”‚
     â”‚                        â”‚                              â”‚
     â”‚                        â–¼                              â”‚
     â”‚               Exception Creation â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                        â”‚
     â”‚                        â–¼
     â””â”€â”€statsâ”€â”€â–¶    Background Processing â”€â”€â–¶ Prefect Flows
```

#### Detailed Event Processing Flow:

1. **Event Generation** (Shopify Mock API):
   ```python
   # Configurable batch generation
   batch_size = random.randint(1001, 1999)  # Via env vars
   
   # 15% probability of problems
   if random.random() < 0.15:
       problem = generate_problem()  # 5 problem types
   
   # Webhook delivery with realistic delay
   await asyncio.sleep(2)  # WEBHOOK_DELAY_SECONDS
   ```

2. **Event Reception** (FastAPI `/events` endpoint):
   ```python
   # Idempotency check via Redis
   event_key = f"event:{tenant}:{source}:{event_id}"
   if await redis.exists(event_key):
       return {"status": "duplicate", "ignored": True}
   
   # Event validation and storage
   event = OrderEvent(**validated_data)
   await db.add(event)
   
   # Real-time SLA evaluation
   exception = await sla_engine.evaluate_sla(event)
   if exception:
       await db.add(exception)
       # Trigger background processing
       asyncio.create_task(process_exception_background(exception.id, tenant))
   ```

3. **SLA Monitoring** (SLA Engine):
   ```python
   # Load cached SLA configuration
   sla_config = await get_sla_config(tenant)  # 1-hour Redis cache
   
   # Time-based analysis
   time_analysis = analyze_timing(event, sla_config)
   
   # Exception creation for violations
   if time_analysis.breach_detected:
       exception = ExceptionRecord(
           tenant=tenant,
           order_id=event.order_id,
           reason_code=time_analysis.reason_code,
           severity=determine_severity(time_analysis)
       )
   ```

4. **AI Analysis Integration**:
   ```python
   # Circuit breaker protected AI calls
   @ai_resilient(max_retries=2, timeout=3)
   async def analyze_exception(exception_data):
       # Token usage tracking
       # Cost monitoring
       # Fallback to rule-based logic on failure
   ```

### 2. Business Process Orchestration

```
Prefect Orchestrator â”€â”€scheduleâ”€â”€â–¶ Business Flows
         â”‚                              â”‚
         â”‚                              â–¼
         â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                    â”‚ Order Processingâ”‚ (Every 30 min)
         â”‚                    â”‚ Exception Mgmt  â”‚ (Every 4 hours)
         â”‚                    â”‚ Billing Mgmt    â”‚ (Daily at 2:00)
         â”‚                    â”‚ Data Enrichment â”‚ (Every 6 hours)
         â”‚                    â”‚ Orchestrator    â”‚ (Every hour)
         â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                              â”‚
         â”‚                              â–¼
         â””â”€â”€monitoringâ”€â”€â—€â”€â”€â”€â”€â”€â”€â”€â”€ Flow Results & Metrics
```

#### Flow Execution Schedule (from `prefect.yaml`):

```yaml
deployments:
  - name: order-processing-final
    schedule:
      interval: 1800  # 30 minutes
    
  - name: business-orchestrator-final
    schedule:
      interval: 3600  # 1 hour
    
  - name: exception-management-final
    schedule:
      interval: 14400  # 4 hours
    
  - name: billing-management-final
    schedule:
      interval: 86400  # 24 hours
    
  - name: data-enrichment-final
    schedule:
      interval: 21600  # 6 hours
```

### 3. Real-time Dashboard Updates

```
Database Changes â”€â”€triggersâ”€â”€â–¶ WebSocket Manager â”€â”€broadcastâ”€â”€â–¶ Dashboard
    â”‚                                â”‚                              â”‚
    â”‚                                â–¼                              â”‚
    â”‚                         Connection Pool                       â”‚
    â”‚                                â”‚                              â”‚
    â”‚                                â–¼                              â”‚
    â””â”€â”€metricsâ”€â”€â–¶            Real-time Metrics â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### WebSocket Integration:
```python
# Connection management
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
    
    async def broadcast_update(self, data: dict):
        for connection in self.active_connections:
            await connection.send_json(data)

# Real-time metric updates
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    # Stream real-time updates
```

---

## ğŸ¼ Orchestration

### Prefect 3.0 Workflow Architecture

The system uses Prefect 3.0 for comprehensive business process orchestration with five main flows:

#### 1. **Business Operations Orchestrator** (Master Flow)

**File**: `flows/business_operations_orchestrator.py`
**Schedule**: Every hour
**Purpose**: Coordinates all business processes with intelligent scheduling

**Key Tasks**:
```python
@task
async def check_system_readiness(tenant: str) -> Dict[str, Any]:
    """Verify system health before operations"""
    readiness_checks = {
        'database_connection': True,
        'api_service': True,
        'redis_cache': True,
        'ai_services': True,
        'webhook_processing': True
    }
    return {"overall_ready": all(readiness_checks.values())}

@task
async def determine_operation_schedule(current_time: datetime, tenant: str) -> Dict[str, Any]:
    """Intelligent scheduling based on business rules"""
    # Business hours consideration
    # System load analysis
    # Maintenance window avoidance
    return schedule_decisions

@flow
async def business_operations_orchestrator(tenant: str = "demo-3pl", mode: str = "auto"):
    """Master orchestration flow"""
    # System readiness check
    readiness = await check_system_readiness(tenant)
    if not readiness["overall_ready"]:
        return {"status": "aborted", "reason": "System not ready"}
    
    # Determine operations to execute
    schedule = await determine_operation_schedule(datetime.utcnow(), tenant)
    
    # Sequential operation execution
    results = []
    if schedule["run_order_processing"]:
        result = await order_processing_pipeline(tenant=tenant)
        results.append({"operation": "order_processing", **result})
    
    if schedule["run_exception_management"]:
        result = await exception_management_pipeline(tenant=tenant)
        results.append({"operation": "exception_management", **result})
    
    if schedule["run_billing_management"]:
        result = await billing_management_pipeline(tenant=tenant)
        results.append({"operation": "billing_management", **result})
    
    return {"status": "completed", "execution_results": results}
```

#### 2. **Order Processing Flow**

**File**: `flows/order_processing_flow.py`
**Schedule**: Every 30 minutes
**Purpose**: Order fulfillment monitoring and processing stage management

**Key Tasks**:
```python
@task
async def monitor_order_fulfillment(tenant: str, lookback_hours: int = 2) -> Dict[str, Any]:
    """Monitor order progress and fulfillment status"""
    # Query recent orders and their status
    # Analyze fulfillment patterns
    # Identify stuck or delayed orders
    return fulfillment_analysis

@task
async def process_eligible_stages(tenant: str) -> Dict[str, Any]:
    """Process order stages that meet dependency requirements"""
    # Check stage dependencies
    # Execute eligible processing stages
    # Update stage status and metrics
    return stage_processing_results

@task
async def monitor_sla_compliance(tenant: str, lookback_hours: int = 2) -> Dict[str, Any]:
    """Monitor SLA compliance for recent orders"""
    # Analyze SLA breach patterns
    # Generate compliance metrics
    # Identify systemic issues
    return sla_analysis

@flow
async def order_processing_pipeline(tenant: str = "demo-3pl", lookback_hours: int = 2):
    """Complete order processing pipeline"""
    fulfillment_results = await monitor_order_fulfillment(tenant, lookback_hours)
    stage_results = await process_eligible_stages(tenant)
    sla_results = await monitor_sla_compliance(tenant, lookback_hours)
    invoice_results = await generate_invoices_for_completed(tenant)
    
    return {
        "fulfillment_monitoring": fulfillment_results,
        "processing_stages": stage_results,
        "sla_monitoring": sla_results,
        "invoice_generation": invoice_results,
        "summary": generate_summary(fulfillment_results, sla_results, invoice_results)
    }
```

#### 3. **Exception Management Flow**

**File**: `flows/exception_management_flow.py`
**Schedule**: Every 4 hours
**Purpose**: AI-powered exception analysis and automated resolution

**Key Tasks**:
```python
@task
async def analyze_exception_patterns(tenant: str, analysis_hours: int = 24) -> Dict[str, Any]:
    """AI-powered pattern analysis of exceptions"""
    # Fetch recent exceptions
    # Analyze patterns and trends
    # Identify root causes
    # Generate insights
    return pattern_analysis

@task
async def prioritize_active_exceptions(tenant: str) -> Dict[str, Any]:
    """Prioritize exceptions based on business impact"""
    # Load active exceptions
    # Apply business rules for prioritization
    # Consider SLA impact and customer tier
    return prioritization_results

@task
async def attempt_automated_resolution(tenant: str, high_priority_exceptions: List[int]) -> Dict[str, Any]:
    """Attempt automated resolution using AI recommendations"""
    # For each high-priority exception
    # Get AI resolution recommendations
    # Apply automated fixes where possible
    # Track success rates
    return automation_results

@flow
async def exception_management_pipeline(tenant: str = "demo-3pl", analysis_hours: int = 24):
    """Complete exception management pipeline"""
    pattern_analysis = await analyze_exception_patterns(tenant, analysis_hours)
    prioritization = await prioritize_active_exceptions(tenant)
    automation_results = await attempt_automated_resolution(tenant, prioritization["high_priority"])
    insights = await generate_exception_insights(tenant, pattern_analysis, automation_results)
    
    return {
        "pattern_analysis": pattern_analysis,
        "exception_prioritization": prioritization,
        "automation_results": automation_results,
        "insights_and_recommendations": insights,
        "summary": generate_exception_summary(pattern_analysis, automation_results)
    }
```

#### 4. **Billing Management Flow**

**File**: `flows/billing_management_flow.py`
**Schedule**: Daily at 2:00 AM
**Purpose**: Automated billing and invoice management

**Key Tasks**:
```python
@task
async def identify_billable_orders(tenant: str, lookback_hours: int = 24) -> Dict[str, Any]:
    """Identify orders ready for billing"""
    # Query completed orders
    # Validate completion status
    # Check billing eligibility
    # Calculate billable operations
    return billable_analysis

@task
async def generate_invoices(tenant: str, billable_orders: List[Dict]) -> Dict[str, Any]:
    """Generate invoices for billable orders"""
    # Create invoice records
    # Calculate amounts based on operations
    # Apply pricing rules and discounts
    # Generate invoice numbers
    return invoice_generation_results

@task
async def validate_invoice_accuracy(tenant: str, generated_invoices: List[Dict]) -> Dict[str, Any]:
    """Validate invoice accuracy and completeness"""
    # Cross-reference with order data
    # Validate calculations
    # Check for missing operations
    # Flag discrepancies
    return validation_results

@flow
async def billing_management_pipeline(tenant: str = "demo-3pl", lookback_hours: int = 24):
    """Complete billing management pipeline"""
    billable_analysis = await identify_billable_orders(tenant, lookback_hours)
    invoice_generation = await generate_invoices(tenant, billable_analysis["billable_orders"])
    validation_results = await validate_invoice_accuracy(tenant, invoice_generation["generated_invoices"])
    adjustment_processing = await process_billing_adjustments(tenant, validation_results["adjustments_needed"])
    billing_report = await generate_billing_report(tenant)
    
    return {
        "billable_analysis": billable_analysis,
        "invoice_generation": invoice_generation,
        "validation_results": validation_results,
        "adjustment_processing": adjustment_processing,
        "billing_report": billing_report,
        "summary": generate_billing_summary(billable_analysis, invoice_generation, validation_results)
    }
```

#### 5. **Data Enrichment Flow**

**File**: `flows/data_enrichment_flow.py`
**Schedule**: Every 6 hours
**Purpose**: Data enrichment and analytics preparation

**Key Tasks**:
```python
@task
async def enrich_order_data(tenant: str, lookback_hours: int = 6) -> Dict[str, Any]:
    """Enrich order data with additional context"""
    # Add customer segmentation data
    # Calculate order complexity scores
    # Enrich with external data sources
    # Update analytics tables
    return enrichment_results

@task
async def analyze_trends_and_patterns(tenant: str, analysis_period_hours: int = 168) -> Dict[str, Any]:
    """Analyze trends and patterns in order data"""
    # Time-series analysis
    # Seasonal pattern detection
    # Performance trend analysis
    # Anomaly detection
    return trend_analysis

@flow
async def data_enrichment_flow(tenant: str = "demo-3pl", lookback_hours: int = 6):
    """Complete data enrichment pipeline"""
    enrichment_results = await enrich_order_data(tenant, lookback_hours)
    trend_analysis = await analyze_trends_and_patterns(tenant, 168)  # 1 week
    performance_optimization = await optimize_performance_metrics(tenant)
    
    return {
        "enrichment_results": enrichment_results,
        "trend_analysis": trend_analysis,
        "performance_optimization": performance_optimization,
        "summary": generate_enrichment_summary(enrichment_results, trend_analysis)
    }
```

### Prefect Deployment Configuration

**File**: `prefect.yaml`

```yaml
# Production deployments with proper scheduling
deployments:
  - name: order-processing-final
    entrypoint: /app/flows/order_processing_flow.py:order_processing_pipeline
    schedule:
      interval: 1800  # 30 minutes
    work_pool:
      name: default-agent-pool
    parameters:
      tenant: "demo-3pl"
      lookback_hours: 2

  - name: business-orchestrator-final
    entrypoint: /app/flows/business_operations_orchestrator.py:business_operations_orchestrator
    schedule:
      interval: 3600  # 1 hour
    work_pool:
      name: default-agent-pool
    parameters:
      tenant: "demo-3pl"
      mode: "auto"

  - name: exception-management-final
    entrypoint: /app/flows/exception_management_flow.py:exception_management_pipeline
    schedule:
      interval: 14400  # 4 hours
    work_pool:
      name: default-agent-pool
    parameters:
      tenant: "demo-3pl"
      analysis_hours: 24

  - name: billing-management-final
    entrypoint: /app/flows/billing_management_flow.py:billing_management_pipeline
    schedule:
      interval: 86400  # 24 hours
    work_pool:
      name: default-agent-pool
    parameters:
      tenant: "demo-3pl"
      lookback_hours: 24

  - name: data-enrichment-final
    entrypoint: /app/flows/data_enrichment_flow.py:data_enrichment_flow
    schedule:
      interval: 21600  # 6 hours
    work_pool:
      name: default-agent-pool
    parameters:
      tenant: "demo-3pl"
      lookback_hours: 6
```

### Flow Execution Dependencies

The flows are designed with proper dependency management:

1. **Business Orchestrator** â†’ Coordinates all other flows
2. **Order Processing** â†’ Generates data for Exception Management
3. **Exception Management** â†’ Uses order data, feeds into Billing
4. **Billing Management** â†’ Depends on completed orders and resolved exceptions
5. **Data Enrichment** â†’ Processes all accumulated data for analytics

---

## ğŸ§© System Components

### 1. FastAPI Application (`app/`)

#### Main Application Module (`app/main.py`)

The FastAPI application provides comprehensive middleware stack and lifecycle management:

```python
@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """Application lifespan manager"""
    # Startup sequence
    init_logging(settings.LOG_LEVEL)
    init_tracing(settings.SERVICE_NAME)
    init_database()
    
    yield
    
    # Shutdown sequence
    await close_database()

def create_app() -> FastAPI:
    """Create and configure FastAPI application"""
    app = FastAPI(
        title="Octup EÂ²A",
        description="SLA Radar + Invoice Guard with AI Exception Analyst",
        version="0.1.0",
        lifespan=lifespan
    )
    
    # Middleware stack (order matters!)
    app.add_middleware(CORSMiddleware, allow_origins=["*"], ...)
    app.add_middleware(CorrelationMiddleware)
    app.add_middleware(TenancyMiddleware, require_tenant=True)
    
    # OpenTelemetry instrumentation
    FastAPIInstrumentor.instrument_app(app)
    
    return app
```

#### Route Structure (`app/routes/`)

**1. Event Ingestion Routes** (`ingest.py`)
```python
@router.post("/events")
async def ingest_event(
    event: IngestEventRequest,
    tenant: str = Depends(get_tenant),
    db: AsyncSession = Depends(get_session)
):
    """Main event ingestion endpoint with idempotency and SLA evaluation"""
    # Redis-based idempotency check
    event_key = f"event:{tenant}:{event.source}:{event.event_id}"
    if await redis.exists(event_key):
        return {"status": "duplicate", "ignored": True}
    
    # Store event
    order_event = OrderEvent(**event.dict(), tenant=tenant)
    db.add(order_event)
    
    # Real-time SLA evaluation
    sla_engine = SLAEngine()
    exception = await sla_engine.evaluate_sla(order_event)
    if exception:
        db.add(exception)
        # Background processing trigger
        asyncio.create_task(process_exception_background(exception.id, tenant))
    
    await db.commit()
    return {"status": "processed", "event_id": event.event_id}
```

**2. Dashboard API Routes** (`dashboard.py`)
```python
@router.get("/api/dashboard/metrics")
async def get_dashboard_metrics(tenant: str = Depends(get_tenant)):
    """Real-time dashboard metrics"""
    return {
        "active_exceptions": await count_active_exceptions(tenant),
        "sla_compliance_rate": await calculate_sla_compliance(tenant),
        "orders_processed_today": await count_orders_today(tenant),
        "ai_analysis_success_rate": await get_ai_success_rate(tenant)
    }

@router.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for real-time updates"""
    await manager.connect(websocket)
    try:
        while True:
            # Send periodic updates
            metrics = await get_real_time_metrics()
            await websocket.send_json(metrics)
            await asyncio.sleep(30)
    except WebSocketDisconnect:
        manager.disconnect(websocket)
```

**3. Exception Management Routes** (`exceptions.py`)
```python
@router.get("/api/exceptions")
async def list_exceptions(
    status: Optional[str] = None,
    severity: Optional[str] = None,
    tenant: str = Depends(get_tenant)
):
    """Filtered exception listing with pagination"""
    
@router.post("/api/exceptions/{exception_id}/resolve")
async def resolve_exception(exception_id: int, resolution: ExceptionResolution):
    """Manual exception resolution with audit trail"""
```

**4. Admin Routes** (`admin.py`)
```python
@router.post("/api/admin/replay-dlq")
async def replay_dead_letter_queue(tenant: str = Depends(get_tenant)):
    """Replay failed events from Dead Letter Queue"""
    
@router.get("/api/admin/system-health")
async def get_system_health():
    """Comprehensive system health check"""
```

#### Core Services (`app/services/`)

**1. SLA Engine** (`sla_engine.py`)
```python
class SLAEngine:
    """Real-time SLA monitoring and breach detection"""
    
    async def evaluate_sla(self, event: OrderEvent) -> Optional[ExceptionRecord]:
        """Evaluate SLA compliance for an order event"""
        # Load cached SLA configuration (1-hour Redis TTL)
        sla_config = await get_sla_config(event.tenant)
        
        # Analyze timing against thresholds
        time_analysis = self.analyze_timing(event, sla_config)
        
        # Create exception for violations
        if time_analysis.breach_detected:
            exception = ExceptionRecord(
                tenant=event.tenant,
                order_id=event.order_id,
                reason_code=time_analysis.reason_code,
                severity=self.determine_severity(time_analysis),
                max_resolution_attempts=sla_config.get("max_resolution_attempts", 3)
            )
            
            # Trigger background AI analysis
            asyncio.create_task(self.analyze_with_ai(exception))
            return exception
        
        return None
```

**2. AI Client** (`ai_client.py`)
```python
class AIClient:
    """OpenRouter API client with resilience patterns"""
    
    @ai_resilient(max_retries=2, timeout=3)
    async def analyze_exception(self, exception_data: Dict) -> Dict:
        """Analyze exception with AI (circuit breaker protected)"""
        # Token usage tracking
        if await self.check_daily_token_limit():
            raise TokenLimitExceeded("Daily token limit reached")
        
        # API call with timeout
        response = await self.client.post(
            f"{self.base_url}/chat/completions",
            json={
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 500
            },
            timeout=self.timeout
        )
        
        # Update token usage
        await self.update_token_usage(response.usage.total_tokens)
        
        return self.parse_ai_response(response)
    
    async def fallback_analysis(self, exception_data: Dict) -> Dict:
        """Rule-based fallback when AI is unavailable"""
        # Implement rule-based classification
        # Return structured response matching AI format
```

**3. Processing Stage Service** (`processing_stage_service.py`)
```python
class ProcessingStageService:
    """Order processing stage management"""
    
    async def process_eligible_stages(self, tenant: str) -> Dict[str, Any]:
        """Process stages that meet dependency requirements"""
        # Query pending stages
        pending_stages = await self.get_pending_stages(tenant)
        
        processed_count = 0
        for stage in pending_stages:
            if await self.check_dependencies_met(stage):
                await self.execute_stage(stage)
                processed_count += 1
        
        return {
            "processed_stages": processed_count,
            "total_pending": len(pending_stages),
            "success_rate": processed_count / len(pending_stages) if pending_stages else 1.0
        }
```

**4. Data Enrichment Pipeline** (`data_enrichment_pipeline.py`)
```python
class DataEnrichmentPipeline:
    """Comprehensive data enrichment for analytics"""
    
    async def enrich_order_data(self, tenant: str, lookback_hours: int) -> Dict[str, Any]:
        """Enrich order data with additional context"""
        # Customer segmentation
        # Order complexity scoring
        # External data integration
        # Analytics table updates
        
        return {
            "orders_enriched": enriched_count,
            "enrichment_types": ["customer_segment", "complexity_score", "external_data"],
            "processing_time_seconds": processing_time
        }
```

#### Data Models (`app/storage/models.py`)

**Core Database Schema**:

```python
class Tenant(Base):
    """Multi-tenant configuration and metadata"""
    __tablename__ = "tenants"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    name: Mapped[str] = mapped_column(String(64), unique=True)
    display_name: Mapped[str] = mapped_column(String(128), nullable=True)
    sla_config: Mapped[Dict[str, Any]] = mapped_column(JSON, nullable=True)
    billing_config: Mapped[Dict[str, Any]] = mapped_column(JSON, nullable=True)
    created_at: Mapped[dt.datetime] = mapped_column(DateTime, default=dt.datetime.utcnow)

class OrderEvent(Base):
    """Order and warehouse events from various sources"""
    __tablename__ = "order_events"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    tenant: Mapped[str] = mapped_column(String(64), ForeignKey("tenants.name"), index=True)
    source: Mapped[str] = mapped_column(String(16))  # shopify, wms, carrier
    event_type: Mapped[str] = mapped_column(String(32), index=True)  # order_created, order_fulfilled
    event_id: Mapped[str] = mapped_column(String(128))
    order_id: Mapped[str] = mapped_column(String(128), index=True)
    occurred_at: Mapped[dt.datetime] = mapped_column(DateTime)
    payload: Mapped[Dict[str, Any]] = mapped_column(JSON)
    correlation_id: Mapped[str] = mapped_column(String(64), nullable=True, index=True)
    
    # Unique constraint to prevent duplicate events
    __table_args__ = (
        UniqueConstraint("tenant", "source", "event_id", name="uq_event"),
        Index("ix_order_events_tenant_order_occurred", "tenant", "order_id", "occurred_at"),
    )

class ExceptionRecord(Base):
    """SLA violations with AI analysis and resolution tracking"""
    __tablename__ = "exceptions"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    tenant: Mapped[str] = mapped_column(String(64), ForeignKey("tenants.name"), index=True)
    order_id: Mapped[str] = mapped_column(String(128), index=True)
    reason_code: Mapped[str] = mapped_column(String(32), index=True)
    status: Mapped[str] = mapped_column(String(16), default="OPEN")
    severity: Mapped[str] = mapped_column(String(16), default="MEDIUM")
    
    # AI analysis fields
    ai_label: Mapped[str] = mapped_column(String(32), nullable=True)
    ai_confidence: Mapped[float] = mapped_column(Float, nullable=True)
    ops_note: Mapped[str] = mapped_column(Text, nullable=True)
    client_note: Mapped[str] = mapped_column(Text, nullable=True)
    
    # Resolution attempt tracking
    resolution_attempts: Mapped[int] = mapped_column(Integer, default=0)
    max_resolution_attempts: Mapped[int] = mapped_column(Integer)
    last_resolution_attempt_at: Mapped[dt.datetime] = mapped_column(DateTime, nullable=True)
    resolution_blocked: Mapped[bool] = mapped_column(default=False)
    resolution_block_reason: Mapped[str] = mapped_column(Text, nullable=True)

class Invoice(Base):
    """Automated invoices for completed orders"""
    __tablename__ = "invoices"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    tenant: Mapped[str] = mapped_column(String(64), ForeignKey("tenants.name"))
    order_id: Mapped[str] = mapped_column(String(128))
    invoice_number: Mapped[str] = mapped_column(String(64), unique=True)
    billable_ops: Mapped[Dict[str, Any]] = mapped_column(JSON)
    amount_cents: Mapped[int] = mapped_column(Integer)
    status: Mapped[str] = mapped_column(String(16), default="DRAFT")

class OrderProcessingStage(Base):
    """Order processing stages with dependency tracking"""
    __tablename__ = "order_processing_stages"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    tenant: Mapped[str] = mapped_column(String(64), ForeignKey("tenants.name"))
    order_id: Mapped[str] = mapped_column(String(128))
    stage_name: Mapped[str] = mapped_column(String(64))
    stage_status: Mapped[str] = mapped_column(String(16), default="PENDING")
    dependencies_met: Mapped[bool] = mapped_column(default=False)
    processing_started_at: Mapped[dt.datetime] = mapped_column(DateTime, nullable=True)
    processing_completed_at: Mapped[dt.datetime] = mapped_column(DateTime, nullable=True)
```

### 2. Shopify Mock API (`demo/shopify-mock/`)

#### Data Generator (`data/generator.py`)
```python
class ShopifyDataGenerator:
    """Realistic Shopify data generation with configurable problems"""
    
    def __init__(self, seed: int = 42):
        """Initialize with deterministic seed for reproducible results"""
        random.seed(seed)
        self.problem_types = [
            'delayed_fulfillment',
            'inventory_shortage', 
            'shipping_delay',
            'payment_issue',
            'address_validation'
        ]
    
    def generate_order_with_problems(self) -> Tuple[Dict, Optional[Dict]]:
        """Generate order with 15% probability of problems"""
        order = self.generate_base_order()
        
        # 15% probability of problems (configurable)
        if random.random() < 0.15:
            problem = self.generate_problem()
            self.apply_problem_to_order(order, problem)
            return order, problem
        
        return order, None
    
    def generate_problem(self) -> Dict[str, Any]:
        """Generate realistic problem scenarios"""
        problem_type = random.choice(self.problem_types)
        
        problems = {
            'delayed_fulfillment': {
                'type': 'delayed_fulfillment',
                'delay_hours': random.randint(2, 48),
                'reason': 'warehouse_backlog'
            },
            'inventory_shortage': {
                'type': 'inventory_shortage',
                'shortage_quantity': random.randint(1, 5),
                'expected_restock_date': (datetime.utcnow() + timedelta(days=random.randint(1, 7))).isoformat()
            },
            # ... other problem types
        }
        
        return problems[problem_type]
```

#### Main API Server (`main.py`)
```python
app = FastAPI(
    title="Shopify Mock API",
    description="Mock Shopify API for Octup EÂ²A demo",
    version="1.0.0"
)

# Configuration from environment
OCTUP_API_URL = os.getenv("OCTUP_API_URL", "http://localhost:8000")
WEBHOOK_DELAY_SECONDS = int(os.getenv("WEBHOOK_DELAY_SECONDS", "2"))
SHOPIFY_DEMO_API_PRODUCE_MIN_ORDERS = int(os.getenv("SHOPIFY_DEMO_API_PRODUCE_MIN_ORDERS", "1001"))
SHOPIFY_DEMO_API_PRODUCE_MAX_ORDERS = int(os.getenv("SHOPIFY_DEMO_API_PRODUCE_MAX_ORDERS", "1999"))

@app.post("/admin/orders/generate")
async def generate_orders(background_tasks: BackgroundTasks):
    """Generate batch of orders and send webhooks"""
    batch_size = random.randint(SHOPIFY_DEMO_API_PRODUCE_MIN_ORDERS, SHOPIFY_DEMO_API_PRODUCE_MAX_ORDERS)
    new_orders = generate_batch_orders(batch_size)
    
    # Send webhooks in background
    for order in new_orders:
        background_tasks.add_task(send_webhook_to_octup, order)
    
    return {
        "generated_orders": len(new_orders),
        "total_orders": len(orders_db),
        "webhook_delay_seconds": WEBHOOK_DELAY_SECONDS
    }

async def send_webhook_to_octup(order_data: Dict):
    """Send realistic webhook to Octup API"""
    webhook_payload = {
        "event_type": "order_created",
        "source": "shopify",
        "event_id": f"evt_{order_data['id']}_{int(time.time())}",
        "order_id": order_data['id'],
        "occurred_at": datetime.utcnow().isoformat(),
        "payload": order_data
    }
    
    # Realistic delay
    await asyncio.sleep(WEBHOOK_DELAY_SECONDS)
    
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{OCTUP_API_URL}/events",
                json=webhook_payload,
                headers={"X-Tenant-Id": "demo-3pl"},
                timeout=10.0
            )
            print(f"âœ… Webhook sent for order {order_data['id']}: {response.status_code}")
        except Exception as e:
            print(f"âŒ Webhook failed for order {order_data['id']}: {e}")
```

### 3. Next.js Dashboard (`dashboard/`)

#### Frontend Architecture
```typescript
// Project structure
src/
â”œâ”€â”€ app/                    # Next.js 14 App Router
â”‚   â”œâ”€â”€ dashboard/         # Main dashboard pages
â”‚   â”œâ”€â”€ exceptions/        # Exception management interface
â”‚   â”œâ”€â”€ analytics/         # Analytics and reporting
â”‚   â””â”€â”€ layout.tsx         # Root layout with providers
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ui/               # Radix UI components (shadcn/ui)
â”‚   â”œâ”€â”€ charts/           # Recharts visualizations
â”‚   â”œâ”€â”€ tables/           # TanStack Table components
â”‚   â””â”€â”€ forms/            # Form components with validation
â”œâ”€â”€ hooks/                # Custom React hooks for API integration
â”œâ”€â”€ lib/                  # Utilities and API clients
â””â”€â”€ types/                # TypeScript type definitions
```

#### Key Components

**1. Real-time Dashboard Metrics**
```typescript
// Real-time metrics with WebSocket integration
const DashboardMetrics = () => {
  const { data: metrics, isLoading } = useMetrics({
    refreshInterval: 30000, // 30 seconds
    tenant: "demo-3pl"
  });

  // WebSocket connection for real-time updates
  useEffect(() => {
    const ws = new WebSocket('ws://localhost:8000/ws');
    
    ws.onmessage = (event) => {
      const update = JSON.parse(event.data);
      // Update metrics in real-time
      updateMetrics(update);
    };

    return () => ws.close();
  }, []);

  return (
    <div className="grid grid-cols-4 gap-4">
      <MetricCard
        title="Active Exceptions"
        value={metrics?.active_exceptions}
        trend={metrics?.exception_trend}
        icon={<AlertTriangle className="h-4 w-4" />}
      />
      <MetricCard
        title="SLA Compliance"
        value={`${(metrics?.sla_compliance_rate * 100).toFixed(1)}%`}
        trend={metrics?.sla_trend}
        icon={<CheckCircle className="h-4 w-4" />}
      />
      <MetricCard
        title="Orders Today"
        value={metrics?.orders_processed_today}
        trend={metrics?.orders_trend}
        icon={<Package className="h-4 w-4" />}
      />
      <MetricCard
        title="AI Success Rate"
        value={`${(metrics?.ai_analysis_success_rate * 100).toFixed(1)}%`}
        trend={metrics?.ai_trend}
        icon={<Brain className="h-4 w-4" />}
      />
    </div>
  );
};
```

**2. Exception Management Interface**
```typescript
// Exception management with filtering and actions
const ExceptionTable = () => {
  const [filters, setFilters] = useState({
    status: 'all',
    severity: 'all',
    tenant: 'demo-3pl'
  });

  const { data: exceptions, isLoading } = useExceptions(filters);

  const handleResolveException = async (exceptionId: number, resolution: string) => {
    try {
      await resolveException(exceptionId, { resolution, resolved_by: 'user' });
      toast.success('Exception resolved successfully');
      // Refresh data
      mutate();
    } catch (error) {
      toast.error('Failed to resolve exception');
    }
  };

  return (
    <div className="space-y-4">
      <ExceptionFilters filters={filters} onFiltersChange={setFilters} />
      
      <DataTable
        data={exceptions}
        columns={exceptionColumns}
        loading={isLoading}
        onResolve={handleResolveException}
        onAnalyze={(id) => router.push(`/exceptions/${id}`)}
      />
    </div>
  );
};
```

**3. API Integration Hooks**
```typescript
// Custom hooks for API integration
export const useMetrics = (options: { refreshInterval?: number; tenant: string }) => {
  return useSWR(
    `/api/dashboard/metrics?tenant=${options.tenant}`,
    fetcher,
    {
      refreshInterval: options.refreshInterval,
      revalidateOnFocus: false,
      dedupingInterval: 10000
    }
  );
};

export const useExceptions = (filters: ExceptionFilters) => {
  const queryString = new URLSearchParams(filters).toString();
  
  return useSWR(
    `/api/exceptions?${queryString}`,
    fetcher,
    {
      refreshInterval: 60000, // 1 minute
      revalidateOnFocus: true
    }
  );
};

// API client with error handling
const fetcher = async (url: string) => {
  const response = await fetch(`${process.env.NEXT_PUBLIC_API_URL}${url}`, {
    headers: {
      'X-Tenant-Id': 'demo-3pl',
      'Content-Type': 'application/json'
    }
  });

  if (!response.ok) {
    throw new Error(`API error: ${response.status}`);
  }

  return response.json();
};
```

### 4. Observability Stack

#### OpenTelemetry Integration (`app/observability/tracing.py`)
```python
def init_tracing(service_name: str):
    """Initialize OpenTelemetry tracing with NewRelic export"""
    resource = Resource.create({
        "service.name": service_name,
        "service.version": "1.0.0",
        "deployment.environment": settings.APP_ENV
    })

    tracer_provider = TracerProvider(resource=resource)

    # OTLP export to NewRelic
    if settings.NEW_RELIC_LICENSE_KEY:
        otlp_exporter = OTLPSpanExporter(
            endpoint=settings.OTEL_EXPORTER_OTLP_ENDPOINT,
            headers={"api-key": settings.NEW_RELIC_LICENSE_KEY}
        )
        tracer_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))

    # Console exporter for development
    if settings.APP_ENV == "dev":
        console_exporter = ConsoleSpanExporter()
        tracer_provider.add_span_processor(BatchSpanProcessor(console_exporter))

    trace.set_tracer_provider(tracer_provider)

def get_tracer(name: str):
    """Get tracer instance for module"""
    return trace.get_tracer(name)

# Usage in services
tracer = get_tracer(__name__)

@tracer.start_as_current_span("sla_evaluation")
async def evaluate_sla(self, event: OrderEvent):
    span = trace.get_current_span()
    span.set_attributes({
        "tenant": event.tenant,
        "order_id": event.order_id,
        "event_type": event.event_type
    })
    # ... SLA evaluation logic
```

#### Prometheus Metrics (`app/observability/metrics.py`)
```python
from prometheus_client import Counter, Histogram, Gauge, Info

# Business metrics
ingest_success_total = Counter(
    'octup_ingest_success_total',
    'Total successful event ingestions',
    ['tenant', 'event_type', 'source']
)

sla_breach_count = Counter(
    'octup_sla_breach_total',
    'Total SLA breaches detected',
    ['tenant', 'reason_code', 'severity']
)

ai_requests_total = Counter(
    'octup_ai_requests_total',
    'Total AI API requests',
    ['provider', 'model', 'operation', 'status']
)

ai_token_usage = Counter(
    'octup_ai_tokens_used_total',
    'Total AI tokens consumed',
    ['provider', 'model']
)

# Performance metrics
ingest_latency_seconds = Histogram(
    'octup_ingest_latency_seconds',
    'Event ingestion latency',
    ['tenant', 'event_type'],
    buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)

sla_evaluation_duration_seconds = Histogram(
    'octup_sla_evaluation_duration_seconds',
    'SLA evaluation processing time',
    ['tenant'],
    buckets=[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]
)

# System state metrics
active_exceptions = Gauge(
    'octup_active_exceptions',
    'Number of active exceptions',
    ['tenant', 'severity']
)

database_connections_active = Gauge(
    'octup_db_connections_active',
    'Active database connections'
)

redis_cache_hit_rate = Gauge(
    'octup_redis_cache_hit_rate',
    'Redis cache hit rate',
    ['cache_type']
)

# Usage in services
@ingest_latency_seconds.labels(tenant=tenant, event_type=event.event_type).time()
async def process_event(event: OrderEvent):
    ingest_success_total.labels(
        tenant=event.tenant,
        event_type=event.event_type,
        source=event.source
    ).inc()
    # ... processing logic
```

#### Structured Logging (`app/observability/logging.py`)
```python
import logging
import json
from datetime import datetime
from typing import Dict, Any

class StructuredFormatter(logging.Formatter):
    """JSON formatter for structured logging"""
    
    def format(self, record: logging.LogRecord) -> str:
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        
        # Add correlation ID if available
        if hasattr(record, 'correlation_id'):
            log_entry["correlation_id"] = record.correlation_id
        
        # Add tenant context if available
        if hasattr(record, 'tenant'):
            log_entry["tenant"] = record.tenant
        
        # Add extra fields
        if hasattr(record, 'extra'):
            log_entry.update(record.extra)
        
        return json.dumps(log_entry)

def init_logging(log_level: str = "INFO"):
    """Initialize structured logging"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('logs/octup.log')
        ]
    )
    
    # Apply structured formatter
    for handler in logging.root.handlers:
        handler.setFormatter(StructuredFormatter())

# Usage with correlation context
logger = logging.getLogger(__name__)

async def process_event(event: OrderEvent, correlation_id: str):
    logger.info(
        "Processing event",
        extra={
            "tenant": event.tenant,
            "order_id": event.order_id,
            "event_type": event.event_type,
            "correlation_id": correlation_id
        }
    )
```

## ğŸš€ Deployment

### Local Development

#### 1. Environment Setup
```bash
# Clone repository
git clone <repository-url>
cd octup/root

# Setup Python environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
pip install -r requirements.txt

# Setup Node.js for dashboard
cd dashboard
npm install
cd ..
```

#### 2. Configuration
```bash
# Copy configuration
cp .env.example .env

# Main settings in .env
DATABASE_URL=postgresql+asyncpg://postgres:postgres@127.0.0.1:54322/postgres
REDIS_URL=redis://localhost:6379
AI_API_KEY=your_openrouter_key
NEW_RELIC_LICENSE_KEY=your_newrelic_key
```

#### 3. Service Startup
```bash
# Start Supabase (PostgreSQL)
supabase start

# Start main stack
./run.sh start

# Check status
./run.sh status
```

#### 4. Data Initialization
```bash
# Apply migrations
alembic upgrade head

# Generate test data
./run.sh demo

# Deploy Prefect flows
cd flows && python -m scripts.utility.deploy_flows
```

### Production Deployment

#### 1. Docker Compose Production
```yaml
# docker-compose.prod.yml
version: '3.8'
services:
  api:
    image: octup-e2a:latest
    environment:
      - APP_ENV=production
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  prefect-worker:
    image: octup-e2a:latest
    command: prefect worker start --pool production-pool
    environment:
      - PREFECT_API_URL=${PREFECT_CLOUD_API_URL}
      - PREFECT_API_KEY=${PREFECT_API_KEY}
    deploy:
      replicas: 2
```

#### 2. Kubernetes Deployment
```yaml
# k8s/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: octup-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: octup-api
  template:
    metadata:
      labels:
        app: octup-api
    spec:
      containers:
      - name: api
        image: octup-e2a:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: octup-secrets
              key: database-url
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
```

#### 3. CI/CD Pipeline
```yaml
# .github/workflows/deploy.yml
name: Deploy to Production
on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Run tests
      run: |
        python -m pytest tests/

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Build Docker image
      run: |
        docker build -t octup-e2a:${{ github.sha }} .
        docker tag octup-e2a:${{ github.sha }} octup-e2a:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
    - name: Deploy to production
      run: |
        kubectl set image deployment/octup-api api=octup-e2a:${{ github.sha }}
        kubectl rollout status deployment/octup-api
```

## ğŸ“Š Monitoring

### Key Metrics

#### 1. Business Metrics
- **SLA Compliance Rate**: Percentage of SLA compliance
- **Exception Resolution Rate**: Speed of exception resolution
- **Order Processing Throughput**: Order processing throughput
- **Invoice Accuracy Rate**: Invoice accuracy
- **AI Analysis Success Rate**: AI analysis success rate

#### 2. Technical Metrics
- **API Response Time**: API response time
- **Database Connection Pool**: Connection pool usage
- **Redis Cache Hit Rate**: Caching efficiency
- **Prefect Flow Success Rate**: Flow execution success rate
- **Error Rate by Service**: Error frequency by service

#### 3. Infrastructure Metrics
- **CPU/Memory Usage**: Resource usage
- **Disk I/O**: Disk operations
- **Network Latency**: Network latency
- **Container Health**: Container health

### Alerting Rules

#### 1. Critical Alerts
```yaml
# Prometheus alerting rules
groups:
- name: octup-critical
  rules:
  - alert: HighErrorRate
    expr: rate(octup_ingest_errors_total[5m]) > 0.1
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "High error rate in event ingestion"

  - alert: SLAComplianceDropped
    expr: octup_sla_compliance_rate < 0.95
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "SLA compliance dropped below 95%"

  - alert: DatabaseConnectionsExhausted
    expr: octup_db_connections_active / octup_db_connections_max > 0.9
    for: 1m
    labels:
      severity: critical
```

#### 2. Warning Alerts
```yaml
- name: octup-warning
  rules:
  - alert: HighExceptionCount
    expr: increase(octup_active_exceptions[1h]) > 100
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "High number of new exceptions"

  - alert: AIServiceDegraded
    expr: rate(octup_ai_failures_total[10m]) > 0.05
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "AI service showing degraded performance"
```

### Dashboards

#### 1. Executive Dashboard
- Overall system overview
- Key business metrics
- SLA compliance trends
- Revenue impact metrics

#### 2. Operations Dashboard
- Real-time system health
- Active exceptions and their status
- Flow execution status
- Performance metrics

#### 3. Technical Dashboard
- Infrastructure metrics
- Database performance
- API response times
- Error rates and logs

## ğŸ”§ Configuration and Setup

### Environment Variables

#### Main Settings
```bash
# Application
APP_ENV=production
SERVICE_NAME=octup-e2a
LOG_LEVEL=INFO

# Database
DATABASE_URL=postgresql+asyncpg://user:pass@host:port/db
DIRECT_URL=postgresql+asyncpg://user:pass@host:port/db

# Redis
REDIS_URL=redis://host:port/db

# AI Services
AI_PROVIDER_BASE_URL=https://openrouter.ai/api/v1
AI_MODEL=google/gemini-2.0-flash-exp:free
AI_API_KEY=your_api_key
AI_MAX_DAILY_TOKENS=200000

# Observability
NEW_RELIC_LICENSE_KEY=your_license_key
OTEL_EXPORTER_OTLP_ENDPOINT=https://otlp.nr-data.net:4317
SENTRY_DSN=your_sentry_dsn

# Prefect
PREFECT_API_URL=http://localhost:4200/api
PREFECT_WORK_POOL=octup-process-pool
```

### Security

#### 1. Authentication and Authorization
```python
# JWT tokens for API access
JWT_SECRET=your-long-random-secret-key

# Multi-tenancy through headers
X_TENANT_ID=demo-3pl
```

#### 2. Network Security
```yaml
# Docker network isolation
networks:
  octup-network:
    driver: bridge
    internal: false  # Only for external API access
```

#### 3. Secrets Management
```bash
# Using Docker secrets or Kubernetes secrets
docker secret create db_password /path/to/password.txt
```

## ğŸ§ª Testing

### End-to-End Validation

The system includes a comprehensive E2E test (`debug/e2e_sanity_check.py`) that:

1. **Checks service availability**
2. **Generates test orders** (30 by default)
3. **Runs all Prefect flows**:
   - order-processing-pipeline
   - exception-management-pipeline
   - billing-management-pipeline
   - business-operations-orchestrator
   - data-enrichment-pipeline
4. **Validates results** of each flow
5. **Checks metrics** and system state

```bash
# Run E2E tests
python debug/e2e_sanity_check.py --start-stack --orders 30

# Quick test
python debug/e2e_sanity_check.py --orders 10 --wait-seconds 10
```

### Unit Tests
```bash
# Run all tests
pytest tests/

# Tests with coverage
pytest --cov=app tests/

# Specific modules
pytest tests/unit/test_sla_engine.py
pytest tests/integration/test_event_processing.py
```

## ğŸ“ˆ Performance and Scaling

### Performance Optimization

#### 1. Database Optimization
```sql
-- Indexes for fast queries
CREATE INDEX CONCURRENTLY idx_order_events_tenant_created
ON order_events(tenant, created_at);

CREATE INDEX CONCURRENTLY idx_exceptions_tenant_status
ON exceptions(tenant, status) WHERE status = 'OPEN';
```

#### 2. Redis Caching
```python
# SLA configurations cached for 1 hour
@cache(expire=3600)
async def get_sla_config(tenant: str) -> Dict:
    return await load_sla_config_from_db(tenant)
```

#### 3. Async Processing
```python
# Parallel event processing
async def process_events_batch(events: List[OrderEvent]):
    tasks = [process_single_event(event) for event in events]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results
```

### Horizontal Scaling

#### 1. API Service
- Stateless design for easy scaling
- Load balancer with health checks
- Auto-scaling based on CPU/memory metrics

#### 2. Prefect Workers
- Multiple workers for parallel processing
- Work pools for load isolation
- Kubernetes Jobs for dynamic scaling

#### 3. Database
- Read replicas for analytical queries
- Connection pooling with PgBouncer
- Partitioning large tables by tenant/date

## ğŸ”„ Operational Procedures

### System Health Monitoring

#### 1. Daily Checks
```bash
# Check status of all services
./run.sh status

# Check Prefect flows
prefect deployment ls
prefect flow-run ls --limit 10

# Check metrics
curl -H "X-Tenant-Id: demo-3pl" http://localhost:8000/api/dashboard/metrics
```

#### 2. Weekly Checks
```bash
# Performance analysis
python debug/performance_analysis.py --days 7

# Data quality check
python debug/data_quality_check.py --tenant demo-3pl

# Update AI models
python scripts/update_ai_models.py
```

### Recovery Procedures

#### 1. Recovery After Failure
```bash
# Restart services
./run.sh stop
./run.sh start

# Check logs
docker logs octup-api
docker logs octup-prefect-worker

# Reprocess DLQ
curl -X POST http://localhost:8000/api/admin/replay-dlq
```

#### 2. Data Recovery
```bash
# Restore from backup
pg_restore -h localhost -p 54322 -U postgres -d postgres backup.sql

# Reprocess events
python scripts/reprocess_events.py --start-date 2025-08-19 --end-date 2025-08-20
```

## ğŸ“š Conclusion

Octup EÂ²A is a modern, scalable platform for managing logistics operations using cutting-edge technologies:

### Key Achievements
- **Real-time SLA monitoring** with automatic violation detection
- **AI-powered exception analysis** with automated resolution
- **Comprehensive business process orchestration** via Prefect
- **Complete observability** with metrics, tracing, and logging
- **Enterprise-grade architecture** with multi-tenancy and security

### Technology Stack
- **Backend**: FastAPI + SQLAlchemy + PostgreSQL + Redis
- **Orchestration**: Prefect 3.0 with Docker deployment
- **Frontend**: Next.js + TypeScript + Tailwind CSS
- **AI**: OpenRouter API with Gemini 2.0 Flash
- **Observability**: OpenTelemetry + NewRelic + Prometheus
- **Infrastructure**: Docker Compose + Kubernetes ready

### Business Value
- **Automation** of routine operations and reduced manual labor
- **Proactive management** of exceptions and SLA violations
- **Accurate billing** with automatic validation
- **Operational transparency** through real-time dashboards
- **Scalability** for business growth

The system is ready for production deployment and can handle high loads while maintaining performance and reliability.

---

## ğŸ“Š Monitoring

### Key Metrics

#### 1. Business Metrics
- **SLA Compliance Rate**: Percentage of orders meeting SLA thresholds
- **Exception Resolution Rate**: Speed and success rate of exception resolution
- **Order Processing Throughput**: Orders processed per hour/day
- **Invoice Accuracy Rate**: Accuracy of automated billing
- **AI Analysis Success Rate**: Success rate of AI-powered analysis

#### 2. Technical Metrics
- **API Response Time**: P50, P95, P99 response times for all endpoints
- **Database Performance**: Connection pool usage, query performance
- **Redis Cache Performance**: Hit rate, memory usage, connection count
- **Prefect Flow Success Rate**: Success rate and duration of workflow executions
- **Error Rate by Service**: Error rates across all system components

#### 3. Infrastructure Metrics
- **Resource Utilization**: CPU, memory, disk usage across containers
- **Network Performance**: Latency, throughput, error rates
- **Container Health**: Health check status, restart counts
- **External Dependencies**: Response times for Supabase, Redis, OpenRouter

### Alerting Configuration

#### 1. Critical Alerts
```yaml
# Prometheus alerting rules
groups:
- name: octup-critical
  rules:
  - alert: HighErrorRate
    expr: rate(octup_ingest_errors_total[5m]) > 0.1
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "High error rate in event ingestion"
      description: "Error rate is {{ $value }} errors per second"
      
  - alert: SLAComplianceDropped
    expr: octup_sla_compliance_rate < 0.95
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "SLA compliance dropped below 95%"
      description: "Current compliance rate: {{ $value }}"
      
  - alert: DatabaseConnectionsExhausted
    expr: octup_db_connections_active / octup_db_connections_max > 0.9
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Database connection pool nearly exhausted"
      
  - alert: AIServiceDown
    expr: up{job="openrouter"} == 0
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "AI service unavailable"
```

#### 2. Warning Alerts
```yaml
- name: octup-warning
  rules:
  - alert: HighExceptionCount
    expr: increase(octup_active_exceptions[1h]) > 100
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "High number of new exceptions"
      description: "{{ $value }} new exceptions in the last hour"
      
  - alert: AIServiceDegraded
    expr: rate(octup_ai_failures_total[10m]) > 0.05
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "AI service showing degraded performance"
      
  - alert: PrefectFlowFailures
    expr: rate(prefect_flow_run_state_total{state="Failed"}[30m]) > 0.1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High Prefect flow failure rate"
```

### Dashboard Configuration

#### 1. Executive Dashboard
- **System Overview**: High-level health and performance indicators
- **Business KPIs**: SLA compliance, exception trends, revenue impact
- **Operational Status**: Service availability, processing volumes
- **Cost Metrics**: AI token usage, infrastructure costs

#### 2. Operations Dashboard
- **Real-time Monitoring**: Live system metrics and alerts
- **Exception Management**: Active exceptions, resolution status
- **Flow Execution**: Prefect workflow status and performance
- **Resource Utilization**: Infrastructure usage and capacity

#### 3. Technical Dashboard
- **Performance Metrics**: Response times, throughput, error rates
- **Database Monitoring**: Query performance, connection usage
- **Cache Performance**: Redis hit rates, memory usage
- **Trace Analysis**: Distributed tracing insights

---

## ğŸ”§ Order of Operations

### System Startup Sequence

1. **Infrastructure Layer**
   ```bash
   # 1. Start Supabase (PostgreSQL)
   supabase start
   
   # 2. Verify database connectivity
   pg_isready -h 127.0.0.1 -p 54322 -U postgres
   
   # 3. Apply database migrations
   alembic upgrade head
   ```

2. **Core Services**
   ```bash
   # 4. Start Redis (if using local)
   docker-compose up -d redis-local
   
   # 5. Start main API service
   docker-compose up -d api
   
   # 6. Verify API health
   curl -f http://localhost:8000/healthz
   ```

3. **Orchestration Layer**
   ```bash
   # 7. Start Prefect server
   docker-compose up -d prefect-server
   
   # 8. Start Prefect worker
   docker-compose up -d prefect-worker
   
   # 9. Deploy flows
   cd flows && python -m scripts.utility.deploy_flows
   ```

4. **Demo and Validation**
   ```bash
   # 10. Start demo services (optional)
   docker-compose --profile demo up -d shopify-mock
   docker-compose --profile dashboard up -d dashboard
   
   # 11. Generate initial data
   ./run.sh demo
   
   # 12. Run end-to-end validation
   python debug/e2e_sanity_check.py --orders 30
   ```

### Business Process Execution Order

1. **Event Ingestion** (Real-time)
   - Shopify Mock generates orders (1001-1999 per batch)
   - Webhooks sent with 2-second delay
   - FastAPI processes events with idempotency checks
   - SLA evaluation triggers exception creation
   - Background AI analysis initiated

2. **Order Processing Flow** (Every 30 minutes)
   - Monitor order fulfillment status
   - Process eligible processing stages
   - Monitor SLA compliance
   - Generate invoices for completed orders

3. **Business Orchestrator** (Every hour)
   - Check system readiness
   - Determine operation schedule
   - Coordinate execution of other flows
   - Generate operational summary

4. **Exception Management Flow** (Every 4 hours)
   - Analyze exception patterns with AI
   - Prioritize active exceptions
   - Attempt automated resolution
   - Generate insights and recommendations

5. **Billing Management Flow** (Daily at 2:00 AM)
   - Identify billable orders
   - Generate invoices
   - Validate invoice accuracy
   - Process billing adjustments
   - Generate billing reports

6. **Data Enrichment Flow** (Every 6 hours)
   - Enrich order data with additional context
   - Analyze trends and patterns
   - Prepare analytics data
   - Optimize performance metrics

### Data Flow Sequence

1. **Event Reception**
   ```
   Shopify Mock â†’ Webhook â†’ FastAPI /events â†’ Validation â†’ Database
   ```

2. **SLA Evaluation**
   ```
   OrderEvent â†’ SLA Engine â†’ Redis Config â†’ Time Analysis â†’ Exception Creation
   ```

3. **AI Analysis**
   ```
   Exception â†’ Background Task â†’ OpenRouter API â†’ AI Response â†’ Database Update
   ```

4. **Flow Orchestration**
   ```
   Prefect Scheduler â†’ Flow Execution â†’ Task Processing â†’ Result Storage
   ```

5. **Dashboard Updates**
   ```
   Database Changes â†’ WebSocket Manager â†’ Real-time Updates â†’ Dashboard
   ```

---

## ğŸ” Implementation Details

### Current Implementation Status

#### âœ… Fully Implemented Features

1. **Event Processing Pipeline**
   - FastAPI application with comprehensive middleware
   - Redis-based idempotency checking
   - Real-time SLA evaluation and exception creation
   - Dead Letter Queue for failed events
   - Background processing with asyncio

2. **AI Integration**
   - OpenRouter API client with circuit breaker pattern
   - Token usage tracking and daily limits
   - Fallback to rule-based logic when AI unavailable
   - Cost monitoring and optimization

3. **Database Schema**
   - Complete PostgreSQL schema with foreign keys
   - Optimized indexes for query performance
   - Alembic migrations for schema management
   - Multi-tenant data isolation

4. **Prefect Workflows**
   - Five comprehensive business flows
   - Proper scheduling and dependency management
   - Error handling and retry logic
   - Comprehensive result tracking

5. **Observability Stack**
   - OpenTelemetry tracing with NewRelic export
   - Prometheus metrics for all components
   - Structured JSON logging with correlation
   - Health checks and monitoring endpoints

#### ğŸš§ Partially Implemented Features

1. **Processing Stages**
   - Database schema exists (`order_processing_stages` table)
   - Service implementation available (`processing_stage_service.py`)
   - **Note**: Stage processing is configurable and can be enabled/disabled
   - **Status**: Working but simplified for demo purposes

2. **Dashboard Interface**
   - Next.js application with modern UI components
   - Real-time WebSocket integration
   - **Note**: Some advanced analytics features are simplified
   - **Status**: Functional with core features implemented

3. **Automated Resolution**
   - AI-powered resolution recommendations implemented
   - **Note**: Actual resolution actions are simulated for safety
   - **Status**: Analysis works, execution is mocked

#### ğŸ”§ Demo/Mock Features

1. **Shopify Mock API**
   - **Purpose**: Realistic data generation for testing
   - **Implementation**: Fully functional FastAPI service
   - **Note**: Simulates real Shopify webhooks with configurable problems

2. **Problem Generation**
   - **Types**: 5 realistic problem scenarios
   - **Probability**: 15% of orders have problems (configurable)
   - **Note**: Problems are simulated but realistic

3. **AI Analysis**
   - **Model**: Uses real Gemini 2.0 Flash via OpenRouter
   - **Limits**: 200,000 tokens/day (free tier)
   - **Fallback**: Rule-based analysis when AI unavailable

### Configuration and Customization

#### Environment Variables
```bash
# Core Application
APP_ENV=dev|production
SERVICE_NAME=octup-e2a
LOG_LEVEL=INFO|DEBUG|WARNING|ERROR

# Database
DATABASE_URL=postgresql+asyncpg://...
DIRECT_URL=postgresql+asyncpg://...

# Redis
REDIS_URL=redis://localhost:6379

# AI Services
AI_API_KEY=your_openrouter_key
AI_MODEL=google/gemini-2.0-flash-exp:free
AI_MAX_DAILY_TOKENS=200000
AI_MODE=smart|full|fallback

# Observability
NEW_RELIC_LICENSE_KEY=your_license_key
OTEL_EXPORTER_OTLP_ENDPOINT=https://otlp.nr-data.net:4317

# Demo Configuration
SHOPIFY_DEMO_API_PRODUCE_MIN_ORDERS=1001
SHOPIFY_DEMO_API_PRODUCE_MAX_ORDERS=1999
WEBHOOK_DELAY_SECONDS=2
```

#### Tenant Configuration
```python
# Multi-tenant SLA configuration stored in database
sla_config = {
    "order_fulfillment_hours": 24,
    "shipping_preparation_hours": 4,
    "payment_processing_minutes": 30,
    "inventory_check_minutes": 15,
    "max_resolution_attempts": 3
}

# Billing configuration per tenant
billing_config = {
    "currency": "USD",
    "rates": {
        "order_processing": 250,  # cents
        "exception_handling": 100,
        "ai_analysis": 50
    }
}
```

### Testing and Validation

#### End-to-End Testing
The system includes comprehensive E2E validation (`debug/e2e_sanity_check.py`):

1. **Service Health Checks**
   - Verify all services are running
   - Check database connectivity
   - Validate API endpoints

2. **Data Generation**
   - Generate configurable number of test orders
   - Trigger webhook processing
   - Verify event ingestion

3. **Flow Execution**
   - Execute all Prefect flows
   - Validate flow results
   - Check data consistency

4. **System Validation**
   - Verify metrics collection
   - Check exception creation
   - Validate AI analysis

```bash
# Run comprehensive E2E test
python debug/e2e_sanity_check.py --start-stack --orders 30 --wait-seconds 15

# Quick validation
python debug/e2e_sanity_check.py --orders 10 --wait-seconds 5
```

### Performance Characteristics

#### Throughput Capabilities
- **Event Ingestion**: 1000+ events/minute with Redis idempotency
- **SLA Evaluation**: Sub-100ms per event with cached configuration
- **AI Analysis**: 2-3 seconds per exception (with 3-second timeout)
- **Database Operations**: Optimized with proper indexing

#### Scalability Considerations
- **Horizontal Scaling**: Stateless API design supports multiple replicas
- **Database**: Connection pooling with configurable limits
- **Redis**: Supports clustering for high availability
- **Prefect**: Multiple workers for parallel flow execution

#### Resource Requirements
- **Development**: 4GB RAM, 2 CPU cores minimum
- **Production**: 8GB RAM, 4 CPU cores recommended per API replica
- **Database**: Separate instance recommended for production
- **Redis**: 1GB memory allocation typical

---

## ğŸ“š Conclusion

Octup EÂ²A represents a comprehensive, production-ready platform for logistics operations management with the following key achievements:

### Technical Excellence
- **Modern Architecture**: FastAPI + Prefect 3.0 + Next.js with comprehensive observability
- **AI Integration**: Real AI-powered analysis with robust fallback mechanisms
- **Enterprise Features**: Multi-tenancy, security, monitoring, and scalability
- **Production Ready**: Docker deployment, health checks, and CI/CD pipeline

### Business Value
- **Operational Efficiency**: Automated SLA monitoring and exception management
- **Cost Optimization**: AI-powered analysis reduces manual intervention
- **Revenue Protection**: Accurate billing and SLA compliance tracking
- **Scalability**: Designed to handle enterprise-scale logistics operations

### Implementation Highlights
- **Real-time Processing**: Sub-second event processing with SLA evaluation
- **Intelligent Automation**: AI-powered exception analysis with 200K token/day capacity
- **Comprehensive Monitoring**: Full observability stack with metrics, tracing, and logging
- **Flexible Deployment**: Docker Compose for development, Kubernetes for production

The system successfully demonstrates how modern technologies can be combined to create a sophisticated logistics management platform that provides real business value while maintaining high technical standards and operational reliability.
